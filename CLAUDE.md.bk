数据准备与嵌入：
step1.py
负责把 .mmd 格式（MultiMarkdown）的科研文献文件做分段、提取元数据（如标题、作者、年份、摘要），然后用 SentenceTransformer（如 all-MiniLM-L6-v2）做文本嵌入，生成向量并用 FAISS 建立向量检索库。
本地 LLM 推理与向量库加载：
step2.py
负责本地加载大语言模型（如 Gemma-3-27B），并实现与向量库的连接，准备后续的 Agent 系统检索和调用。这里用到了 LangChain 的 FAISS 向量库封装，也有自定义的 LLM client（CustomGemmaClient）用于统一推理接口。
多 Agent 自动评审与改写建议生成：
step3.py
负责核心的“多智能体”自动化评审流程。包括内容技术性、结构、可读性、引用准确性等的自动分析、打分和建议列表生成，并通过 moderator agent 综合各方建议，给出最终改写指令。
自动改写、结构化输出与可视化：
rewrite_function.py
封装了 LLM 的调用细节与多轮对话 prompt 处理，支持大模型输出文本的清理、结构化拆分等，专注于自动改写、内容生成与输出格式处理。
step4.py
进一步利用大模型自动为每一篇文档生成知识结构图（如 Mermaid 图），实现可视化，并支持批量处理、日志记录等工程功能。
自动评分评价：
final_evaluation_openrouter.py
用于对生成内容进行“终极自动化评价”（如技术深度、语言复杂度、结构、引用准确性等维度的评分），部分调用了外部 API（如 OpenRouterDeepSeek）如果可以的话这里可以改成本地模型 或者我也可以提供一下我openrouter的apikey 里面的免费模型还是能用来测试的
微调：
fine_tune.py
用于自有数据集上的 LLM 微调。支持从 markdownjson 结构提取文本，构建 HuggingFace 数据集，结合 LoRA、BitsAndBytes 做高效微调。


问题：
主体部分是用本地LLM根据文献生成报告 然后配合agent作改动 最后生成一份报告 然后对本地那个模型作微调 ，agent可以用小模型 但基本上都要保证在本地
主要改agent部分 利用ag2的包实现更多功能 但是要先能够在超算上面跑通 基本上要本地模型的话只能用VLLM（？），sbatch到slurm节点 系统是linux 英伟达CUDA集群，我的权限最多能调用到2张A100 80G的GPU，但是太拥挤了 不一定能排得到队 不拥挤的节点我能申请到18块A100 80G，但是确实是太小了。 需要我测试的时候可以把本地主模型和agent部分的模型统一换成比较小的模型 满足18块A100 80G要求 
原版使用的gemma3-27B 但是好像是4-bit量化后的 我个人比较想把这个换成Qwen3-32B的模型 或者那种上下文长度够 像Nemotron之类的 且支持我硬件条件的模型 不知道用VLLM能不能实现GPU并行 如果可以 那意味着可以往更大一点的模型靠（我可以尝试申请Docker权限）

诉求：
原版的原始文献是有关物理类的 我最后要做一个关于化学化工方面的微调模型 原版mmd文献也是用pdf下载然后docling去转换的 解决不了图片和公式的问题 图片在这里可能就不考虑了 不知道您能不能想想办法解决一下公式问题 就一些化学公式啥的 可能会用到 就是换成mmd之后也要有类似latex格式的公式在里面 然后一起输送给本地模型 我看ag2里面也有一个DocAgent  不清楚有没有用

把原先流程中agent评分那些地方替换掉 具体可不可以用ag2里面的groupchat和captainagent后续开发可以沟通 我对整体结构部分要修改的地方想法不多 还得请您多交流 然后想点点子 输出内容就是一个有关我提供的文献内容作结构化的报告 和一个被微调好的本地模型 只要后续的输出结果和模型微调效果差不多就行 

Step1、2那些对文本分段和嵌入的工作以及step4里面做向量图的 如果能够兼容的话也就不用换了 主要是要突出agent部分的创新 这个我是真不好想。