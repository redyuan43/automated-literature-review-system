

This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program (IITP-2021-2020-0-01461) supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation), and also supported by the Super Computer Development Leading Program of the National Research Foundation of Korea(NRF) funded by the MSIT, Korea (No. : 2020M3H6A1085498).

_Department of Electrical Engineering, POSTECH, Pohang, Korea_

dyrin@postech.ac.kr, kwon3ohj@postech.ac.kr, youngjoo.lee@postech.ac.kr

## I Introduction

Machine-learning (ML) based image signal processing (ISP) has been continuously developed for constructing a high-quality RGB-colored image from the raw camera-sensor inputs [1]. The conventional ISP operations such as demosaic, denoise, and color correction have been successfully replaced by the series of convolutions with trained weights, aiming to obtain high-quality photographic results from the raw camera inputs of resource-limited mobile devices [2, 3]. However, existing ML models generally focus on quality enhancements, which cannot be directly operated with traditional ML accelerators due to slow and power-hungry realizations. This is mainly caused by the reduced resource utilization of 2D array-structured multipliers for mapping the given ML-based ISP models, as the previous array structures are normally optimized for the convolutional neural networks (CNNs) for image classification [4, 5]. However, it is impractical to test all the accelerator configurations for finding the energy-optimized design parameters. Therefore, developing an efficient array structure for properly placing multipliers in a systematic way is required, which is fully dedicated to the ML-based ISP models.

Targeting the recently advanced networks, we present an algorithm-hardware co-optimization strategy for developing the cost-efficient accelerator implementation. By using the open-source architectural simulator for evaluating CNN accelerators using systolic-array structures, we newly define the hardware-level performance metrics to measure the processing speed and the required energy consumption. To find the efficient array configuration in terms of the number of multipliers and the array dimension, we first evaluate various array dimensions under the pre-determined number of multipliers, finding the initial configuration. Then, three array-scaling methods are investigated to increase the number of multipliers for selecting the array configuration, achieving the best efficiency. For the selected array structure, we also introduce the algorithm-level refinement scheme that can enhance the overall efficiency by modifying the internal layer-wise parameters with negligible quality drops of image outputs. Experimental results show that the systolic ML-based ISP accelerator from the proposed co-design strategy further improves the energy efficiency by 51%, still generating high-quality images compared to the baseline accelerator without applying the co-optimization approach.

## II Backgrounds

### _Network Models for ML-based ISP_

Like the typical CNN structures [7, 8], the ML-based ISP models have adopted convolution layers for training spatial features that can enhance the quality of image processing [9, 10]. Considering the computational complexity and the output image quality, we selected two state-of-the-art models for case studies of big and small networks. More specifically, CSANet from [6] is used for representing a small-sized ML-based ISP model, consisting of 15 2D convolution layers, two depth-wise convolution layers, and one transpose convolution layer. On the other hand, for the performance-oriented large-sized model, we select Eureka network in [2], performing a series of residual inception with squeeze and excite (RISE) modules andefficient spatial pyramid (ESPy) modules. Table I summarizes target ML-based ISP networks in this work.

In order to evaluate the algorithm-level performance, as described in [11], we adopted Zurich RAW to RGB dataset that includes 48,043 image pairs between raw image-sensor data of Huawei P20 mobile phone and high-quality RGB-colored pictures of Canon 5D MarkIV camera. As described in Table I, CSANet model definitely requires few parameters with compact model size compared to Eureka network, expecting the energy-efficient processing suitable for resource-limited devices. If we consider the quality of converted images, which is measured by using well-known PNSR and SSIM metrics [12, 13, 14], however, it is clear that the large-sized Eureka model provides superior results to the small-sized counterpart. For developing the cost-efficient accelerator architecture, therefore, it is required to select the proper network depending on the requirements from applications as well as the available hardware resources, improving the overall processing efficiency by fully exploiting the internal model configurations.

### _Architectural Simulator of Systolic ML Accelerators_

To evaluate the efficiency of different accelerator configurations, we used the open-source SCALE-Sim framework reported in [15]. In the SCALE-sim simulator, a number of multipliers are arranged by using the concept of a systolic array for implementing ML accelerator, which performs the weight-stationary data flow by allocating trained weights along the column-wise direction. In this work, for the sake of simplicity, we assume that the dedicated accelerator includes the internal SRAM buffer enough to store the trained weights of each layer at a time, removing the additional cycles during the layer-wise processing caused by buffer size limitations. By setting the array dimension, we can obtain several hardware-aware performances for the given ML model in terms of the number of operating cycles, the hardware utilization of systolic array, and the amount of SRAM/DRAM accesses. However, it is hard to directly map these results to the top-level efficiency of the ML accelerator. Based on the simulation results from SCALE-Sim, in this work, we introduce a systematic way for finding the systolic array structure for efficiently operating the given ML-based ISP model. In addition, we also present an advanced way to manipulate the network model for the selected array architecture, further improving the efficiency even supporting the attractive image results.

## III Proposed Method

### _Evaluation Metrics for Finding the Efficient Architecture_

Based on the performance results from the SCALE-Sim framework, we define some hardware-oriented metrics to evaluate the efficiency of the accelerator architecture. Considering the processing sequence inside of the systolic array, the computing-aware metric \(M_{\text{C}}\) is firstly introduced to provide a comprehensive grasp over the number of operating cycles and the hardware utilization, which is formulated as follows.

\[M_{\text{C}}=\sum_{i=1}^{l_{\text{ms}}}l_{i}\frac{c_{i}}{u_{i}}, \tag{1}\]

where \(l_{\text{max}}\), \(c_{i}\), and \(u_{i}\) are the maximum number of layers, the number of cycles for the \(i\)-th layer, and the average utilization for performing the \(i\)-th layer, respectively. To consider the layer-wise complexity, we use \(l_{i}\) to denote the ratio of the number of multiply-accumulate (MAC) operations in the \(i\)-th layer to the total MAC counts in the given ML model. Hence, decreasing \(M_{\text{C}}\) leads to better efficiency by reducing the operation speed or redundant resources.

In terms of energy consumption, which is also a critical point for evaluating the ML accelerator [16, 17], we present the energy-aware metric for a specific array structure. As reported in various studies [18, 19], there exist three major energy-hungry parts in ML accelerators; the massive-parallel multiplier-based computing engine, the on-chip SRAM buffer requests, and the external DRAM accesses. Normalized to the energy consumption of a single multiplication, as reported in [18], we introduce the energy-aware metric \(M_{\text{E}}\) as follows.

\[M_{\text{E}}=\alpha\times a_{\text{D}}+\beta\times a_{\text{S}}+r\times k_{ \text{R}}k_{\text{C}}, \tag{2}\]

where \(\alpha\) and \(\beta\) denote the relative energy consumption of unit data transfer at the external DRAM and the internal SRAM, respectively, which are the normalized values with respect to the energy consumption of a single multiplier operation. To consider the energy consumption by memories, \(a_{\text{D}}\) and \(a_{\text{S}}\) denote the total size of data movements from DRAM and SRAM, respectively. In general, the DRAM access requires much more energy than the other parts, e.g., \(\alpha=200\) and \(\beta=6\) in 65nm CMOS technology as shown in [16, 18]. The value \(r\) in (2) represents the number of cycles activating multipliers to consider the energy consumption caused by the systolic array. Similar to the computing-aware metric, reducing \(M_{\text{E}}\) directly leads to efficient operations. Finally, we define the overall metric \(M_{\text{O}}=(M_{\text{C}}\times M_{\text{E}})\), strongly related to the actual energy efficiency by taking both the processing speed and the energy consumption. Utilizing \(k_{\text{T}}=(k_{\text{R}}\times k_{\text{C}})\) multipliers, as a result, the optimal array structure of \(k_{\text{R}}\times k_{\text{C}}\) dimension should minimize \(M_{\text{O}}\). However, it is still challenging to solve this optimization problem for obtaining the cost-optimized design directly, requiring a more systematic way to find the cost-efficient array dimensions for the target ML model.

### _Selecting the Cost-Efficient Array Configuration_

Fig. 1 conceptually depicts the processing flow of the proposed systematic strategy for designing the cost-efficient ML-based ISP accelerator, composed of two co-optimization phases; the algorithm-aware array design phase and the hardware-aware model refinement phase. At the array design phase, we first set the initial array structure by limiting the number of total multipliers to \(k_{\text{T}}^{\text{init}}\). For the practical implementation, we also set each parameter to be power of two, i.e., \(k_{\text{R}}=2^{r}\) and \(k_{\text{C}}=2^{c}\). By changing \(r\) and \(c\), from the SCALE-Sim framework, the overall metric \(M_{\text{O}}\) is repeatedly evaluated by calculating \(M_{\text{C}}\) and \(M_{\text{E}}\), finding the initial configuration of \(k_{\text{R}}^{\text{init}}\) and \(k_{\text{C}}^{\text{init}}\) minimizing \(M_{\text{O}}\) without increasing the number of initial multipliers.

Based on the initial configuration, we then increase the number of total multipliers to find more efficient array structures. As illustrated in Fig. 2, more precisely, we categorize three scaling ways to extend the initial array; 1) 1D-vertical (1DV) scaling, 2) 1D-horizontal (1DH) scaling, and 3) 2D scaling. The 2D scaling arranges additional multipliers while maintaining the aspect ratio of the initial array, which can be regarded as a straightforward array scaling. In 1DV and 1DH scaling methods, on the other hand, we increase the number of multipliers by appending rows and columns to the initial array structure, respectively, changing the aspect ratio to find more suitable configurations. By utilizing more multipliers, it is expected to reduce the overall metric for a while as we can reduce the processing cycles as well as the redundant buffer accesses caused by the insufficient number of multipliers in the initial array. If we increase \(k_{\text{T}}\) too much, however, the value of \(M_{\text{O}}\) rapidly increases due to the severe utilization drops. As reported in [20, 21], the utilization starts to decrease when the unrolled operations in a particular layer cannot be covered by the systolic array properly, e.g., the number of rows exceeds the number of the unrolled filter size, or the number of columns exceeds the number of output channels. Regardless of the scaling methods, therefore, there should be a certain limitation of increasing \(k_{\text{T}}\), which is no longer effective for improving the overall processing efficiency.

Without applying the straightforward 2D scaling directly, in the proposed work shown in Fig. 1, we first perform the 1DV scaling to allow more rows (increasing \(k_{\text{R}}\)) until we meet the rapid increment of \(M_{\text{O}}\). It is also possible to roughly estimate the turning point of utilization drops by investigating the size of input channels and filters for each layer. As the systolic accelerator exploits the unrolled MAC operations, the number of weights assigned to the column-wise direction can be computed in advance so that we can check only a few \(k_{\text{R}}\) candidates to cover the most of layers not to cause redundant rows. By evaluating \(M_{\text{O}}\) for these candidates, we can simply determine \(k_{\text{R}}^{\text{scale}}\), i.e., the number of rows after performing the 1DV scaling. Then, the 1DH scaling is applied by appending additional columns to the systolic array. In this case, similar to the 1DV scaling, some candidates of \(k_{\text{C}}\) are pre-selected by checking the number of output channels in each layer. More specifically, the number of columns should be large enough to cover all the output channels at the same time, but also should be selected to reduce the disabled columns as many as possible. In our systematic strategy in Fig. 1, we carefully find the most promising \(k_{\text{C}}^{\text{scale}}\) value by evaluating the overall metric \(M_{\text{O}}\). As a result, the proposed step-by-step 1D scaling approach finally selects the (\(k_{\text{R}}^{\text{scale}}\), \(k_{\text{C}}^{\text{scale}}\)) pair for the cost-efficient ML-based ISP operations.

### _Adjusting ML Model for Maximizing the Efficiency_

As described in Fig. 1, after performing the algorithm-aware hardware design, we proceed with the model refinement phase based on the selected \(k_{\text{R}}^{\text{scale}}\times k_{\text{C}}^{\text{scale}}\) systolic array, further improving the overall efficiency of the accelerator. In this phase, we check the layer-wise parameters of the original ML model, i.e., the number of input/output channels, and the size of the filter. More specifically, we focus on the layers whose \(M_{\text{C}}\) suffer from the low-level resource utilization. If a particular layer uses much fewer weights than \(k_{\text{R}}\), we change the filter size or the number of input channels, allocating more weights for reducing the number of disabled rows. After adjusting the filter size, if inefficient layer-wise processing still exists, the channel modification step is followed. When there exists a mismatch between the value of \(k_{\text{C}}\) and the number of channels, we try to adjust the number of output channels to recover the resource utilization for activating the columns as many as possible. Adjusting the layer-wise parameters requires additional re-training of the original model, minimizing the impacts on the quality of generated images. As we only modify a few layers with the extreme low-level utilization, the algorithm-level performance of the new model is almost similar to that of the original one. On the other hand, in terms of the processing efficiency, the proposed co-optimization method allows to fully enjoy the systolic array design with the modified ML model by removing disabled multiplications, achieving the cost-efficient accelerator designs with subtle degradation on the output image quality.

## IV Experimental Results

To validate the proposed methodology, two case studies were performed to develop the dedicated ML-based ISP accelerators for CSANet and Eureka networks in Table I, where the initial arrays are determined as \(8\times 8\). Fig. 3 shows how the overall metric \(M_{\text{O}}\) is gradually optimized by adjusting the array configuration. As we expected, there exists an optimal point of \(k_{\text{T}}\) that achieves the minimum \(M_{\text{O}}\) depending on the target ML-based ISP model. When we use the CSANet model, as depicted in Fig. 3(a), 1,024 multipliers are enough to provide the most efficient processing, whereas the accelerator for

Fig. 1: Co-optimization flow for designing the proposed accelerator.

Fig. 2: Three scaling methods to increase the number of multipliers.

the Eureka network should utilize 16,384 multipliers to maximize efficiency, as shown in Fig. 3(b). This is mainly caused by the initial complexity of the given ML model, requiring more hardware resources for complex networks to develop the cost-efficient accelerator. Compared to the straightforward 2D scaling, as shown in Fig. 3, it is clear that the proposed systematic method finds more attractive array configurations when we necessitate the same number of multipliers. For the case of the Eureka model, for example, the optimal array configuration becomes \((k_{\text{e}}^{\text{scale}},k_{\text{C}}^{\text{scale}})=(512,32)\), reducing the achievable \(M_{\text{O}}\) by 83% compared to that from the 2D scaling method using a \(128\times 128\) systolic array.

In addition to the optimal systolic structure, as summarized in Table II, we performed the proposed hardware-aware model refinement phase on the CSANet network, further optimizing the overall efficiency. During the refinement process, considering the \(64\times 16\) systolic array from the algorithm-aware array design phase, we first modified a layer with \(5\times 5\) filters to have \(7\times 7\) ones, reducing the number of disabled rows as many as possible. Then, two layers with \(3\times 3\) filters and 16 input channels were changed to have 14 input channels to control the number of activated rows and columns, maximizing the average utilization. By carefully adjusting the number of channels, when compared to the original design, the size of unrolled filters is slightly decreased for achieving better efficiency. For the modified CSANet model, note that the retraining step is followed to recover the algorithm-level performance. For the same systolic accelerator architecture, as a result, the proposed modified model reduces the value of \(M_{\text{O}}\) by 33% compared to the original network, still providing almost similar PSNR and SSIM values as described in Table II. Compared to the ground-truth image from the high-end camera [11], Fig. 4 illustrates sample images generated by two network models, showing that the hardware-aware model refinement generates a comparable image to the original CSANet model.

We also designed prototype ML-based ISP accelerators in a 28nm CMOS technology, validating that the proposed optimization methods actually improve the energy efficiency of the systolic accelerator. As shown in Table II, the proposed 1DV+1DH scaling results lower \(M_{\text{O}}\) than the straightforward 2D scaling, directly improving the core-level energy efficiency by 27% with the algorithm-aware array structure. The hardware-aware algorithm refinement further reduces \(M_{\text{O}}\) by 33% with the maximally-utilized systolic array, finally achieving the energy efficiency of 709 GOPs/W, which is 51% better than the straightforward design as depicted in Table II. As a result, the proposed algorithm-hardware co-optimization approach successfully allows to develop the high-quality and cost-efficient ML-based ISP accelerator in a systematic way without testing all the possible architecture candidates.

## V Conclusion

In this paper, we have presented a systematic way of constructing the cost-efficient accelerator architecture for ML-based ISP applications. Based on the open-source simulation framework, the proposed method newly defines several metrics to evaluate the efficiency of systolic accelerators. Then the algorithm-hardware co-optimization strategy is applied to find the most promising configurations on both the hardware architecture and the model parameters. Case studies on recent ML-based ISP models show that the proposed method always provides the cost-efficient array dimensions compared to the straightforward method and also offers an interesting way to adjust the original network for further optimizing the efficiency of ISP operations under the given accelerator architecture.

Fig. 4: Sample images from original and modified CSANet networks.

Fig. 3: Optimizing hardware-aware metrics by finding the cost-efficient array dimension dedicated to (a) CSANet [6] and (b) Eureka [2] models.

[MISSING_PAGE_EMPTY:1477]