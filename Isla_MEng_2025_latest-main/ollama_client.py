# ollama_client.py - Ollamaå®¢æˆ·ç«¯å°è£…

import requests
import json
import time
import logging
from typing import Dict, Any, List, Optional, Generator
from config import OLLAMA_CONFIG, MODEL_CONFIG, SYSTEM_CONFIG

logger = logging.getLogger(__name__)

class OllamaClient:
    """Ollama APIå®¢æˆ·ç«¯"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.base_url = OLLAMA_CONFIG.base_url
        self.timeout = OLLAMA_CONFIG.timeout
        self.session = requests.Session()
        
        # åº”ç”¨è‡ªå®šä¹‰é…ç½®
        if config:
            for key, value in config.items():
                if hasattr(OLLAMA_CONFIG, key):
                    setattr(OLLAMA_CONFIG, key, value)
    
    def generate(self, 
                model: str,
                prompt: str,
                system: str = None,
                temperature: float = None,
                max_tokens: int = None,
                top_p: float = None,
                stream: bool = False) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            model: æ¨¡å‹åç§°
            prompt: ç”¨æˆ·è¾“å…¥
            system: ç³»ç»Ÿæç¤º
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            top_p: top_på‚æ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            
        Returns:
            ç”Ÿæˆçš„å“åº”
        """
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": {}
        }
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if system:
            data["system"] = system
        
        # æ·»åŠ å‚æ•° (ä»…åœ¨æ˜ç¡®æŒ‡å®šæ—¶è®¾ç½®ï¼Œå¦åˆ™ä½¿ç”¨Ollamaé»˜è®¤å€¼)
        if temperature is not None:
            data["options"]["temperature"] = temperature
            
        if max_tokens is not None:
            data["options"]["num_predict"] = max_tokens
            
        if top_p is not None:
            data["options"]["top_p"] = top_p
        
        try:
            if stream:
                return self._stream_generate(data)
            else:
                return self._single_generate(data)
                
        except Exception as e:
            logger.error(f"Ollamaç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def _single_generate(self, data: Dict) -> Dict[str, Any]:
        """å•æ¬¡ç”Ÿæˆ"""
        url = f"{self.base_url}/api/generate"
        
        response = self.session.post(
            url, 
            json=data, 
            timeout=self.timeout
        )
        response.raise_for_status()
        
        result = response.json()
        return {
            "response": result.get("response", ""),
            "done": result.get("done", False),
            "total_duration": result.get("total_duration", 0),
            "load_duration": result.get("load_duration", 0),
            "prompt_eval_count": result.get("prompt_eval_count", 0),
            "eval_count": result.get("eval_count", 0),
        }
    
    def _stream_generate(self, data: Dict) -> Generator[Dict[str, Any], None, None]:
        """æµå¼ç”Ÿæˆ"""
        url = f"{self.base_url}/api/generate"
        
        with self.session.post(
            url, 
            json=data, 
            timeout=self.timeout,
            stream=True
        ) as response:
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    try:
                        chunk = json.loads(line.decode('utf-8'))
                        yield chunk
                    except json.JSONDecodeError:
                        continue
    
    def chat(self,
             model: str,
             messages: List[Dict[str, str]],
             temperature: float = None,
             max_tokens: int = None,
             stream: bool = False) -> Dict[str, Any]:
        """
        å¯¹è¯æ¥å£
        
        Args:
            model: æ¨¡å‹åç§°
            messages: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            
        Returns:
            å¯¹è¯å“åº”
        """
        
        data = {
            "model": model,
            "messages": messages,
            "stream": stream,
            "options": {}
        }
        
        # æ·»åŠ å‚æ•° (ä»…åœ¨æ˜ç¡®æŒ‡å®šæ—¶è®¾ç½®ï¼Œå¦åˆ™ä½¿ç”¨Ollamaé»˜è®¤å€¼)
        if temperature is not None:
            data["options"]["temperature"] = temperature
            
        if max_tokens is not None:
            data["options"]["num_predict"] = max_tokens
        
        try:
            url = f"{self.base_url}/api/chat"
            
            if stream:
                return self._stream_chat(url, data)
            else:
                response = self.session.post(url, json=data, timeout=self.timeout)
                response.raise_for_status()
                result = response.json()
                
                return {
                    "message": result.get("message", {}),
                    "done": result.get("done", False),
                    "total_duration": result.get("total_duration", 0),
                    "load_duration": result.get("load_duration", 0),
                    "prompt_eval_count": result.get("prompt_eval_count", 0),
                    "eval_count": result.get("eval_count", 0),
                }
                
        except Exception as e:
            logger.error(f"Ollamaå¯¹è¯å¤±è´¥: {e}")
            raise
    
    def _stream_chat(self, url: str, data: Dict) -> Generator[Dict[str, Any], None, None]:
        """æµå¼å¯¹è¯"""
        with self.session.post(
            url, 
            json=data, 
            timeout=self.timeout,
            stream=True
        ) as response:
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    try:
                        chunk = json.loads(line.decode('utf-8'))
                        yield chunk
                    except json.JSONDecodeError:
                        continue
    
    def list_models(self) -> List[Dict[str, Any]]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            url = f"{self.base_url}/api/tags"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            result = response.json()
            return result.get("models", [])
            
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def pull_model(self, model: str) -> bool:
        """æ‹‰å–æ¨¡å‹"""
        try:
            url = f"{self.base_url}/api/pull"
            data = {"name": model}
            
            response = self.session.post(url, json=data, timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶
            response.raise_for_status()
            
            return True
            
        except Exception as e:
            logger.error(f"æ‹‰å–æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            models = self.list_models()
            return len(models) >= 0
        except:
            return False

# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
ollama_client = OllamaClient()

# ä¾¿æ·å‡½æ•°
def generate_text(prompt: str, 
                 model: str = None, 
                 system: str = None,
                 task_type: str = None,
                 **kwargs) -> str:
    """
    ä¾¿æ·çš„æ–‡æœ¬ç”Ÿæˆå‡½æ•°
    
    Args:
        prompt: è¾“å…¥æç¤º
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ä¸»æ¨¡å‹
        system: ç³»ç»Ÿæç¤º
        task_type: ä»»åŠ¡ç±»å‹ï¼Œç”¨äºè·å–ç‰¹æ®Šé…ç½®
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬
    """
    
    if model is None:
        model = MODEL_CONFIG.main_model
    
    # åº”ç”¨ä»»åŠ¡ç‰¹å®šé…ç½®
    if task_type and task_type in MODEL_CONFIG.task_configs:
        task_config = MODEL_CONFIG.task_configs[task_type]
        for key, value in task_config.items():
            if key not in kwargs:
                kwargs[key] = value
    
    result = ollama_client.generate(
        model=model,
        prompt=prompt,
        system=system,
        **kwargs
    )
    
    return result["response"]

def chat_with_model(messages: List[Dict[str, str]], 
                   model: str = None,
                   **kwargs) -> str:
    """
    ä¾¿æ·çš„å¯¹è¯å‡½æ•°
    
    Args:
        messages: å¯¹è¯å†å²
        model: æ¨¡å‹åç§°
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        æ¨¡å‹å›å¤
    """
    
    if model is None:
        model = MODEL_CONFIG.small_model  # å¯¹è¯é»˜è®¤ä½¿ç”¨å°æ¨¡å‹
    
    result = ollama_client.chat(
        model=model,
        messages=messages,
        **kwargs
    )
    
    return result["message"]["content"]

if __name__ == "__main__":
    # æµ‹è¯•è¿æ¥
    client = OllamaClient()
    
    print("ğŸ” æ£€æŸ¥Ollamaè¿æ¥...")
    if client.health_check():
        print("âœ… Ollamaè¿æ¥æ­£å¸¸")
        
        # åˆ—å‡ºå¯ç”¨æ¨¡å‹
        models = client.list_models()
        print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {[m['name'] for m in models]}")
        
        # æµ‹è¯•ç”Ÿæˆ
        if models:
            test_model = models[0]['name']
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {test_model}")
            
            response = generate_text(
                prompt="Hello, how are you?",
                model=test_model,
                max_tokens=50
            )
            print(f"ğŸ’¬ æµ‹è¯•å“åº”: {response}")
        
    else:
        print("âŒ Ollamaè¿æ¥å¤±è´¥")