## Codebase Explanation: step3.py

This document provides an in-depth analysis of the `step3.py` script. This script functions as an iterative refinement and quality assurance stage for academic-style chapters, presumably generated by `step2.py`. It employs a multi-agent system built with the AutoGen library, along with an evaluation framework and a rewriting function, to systematically enhance the quality of each section within the chapters.

### Executive Summary

`step3.py` takes chapter data (in JSON format from `initial_chapters/`) as input and processes each section (except "REFERENCES") through an iterative review and rewrite cycle. It uses a team of specialized AutoGen agents (Technical Accuracy, Clarity, Structure, and Fact-Checking) to critique the content. A Moderator agent then synthesizes these critiques into a consolidated list of improvement points. Based on these points, the `rewrite_function.py` module is invoked to revise the text. An `evaluation_framework.py` (using `final_evaluation.py` functions) assesses the quality of the text after each iteration against predefined metrics (technical depth, clarity, structure, and citation accuracy). The process continues for a set number of iterations or until quality thresholds are met. Finally, the script saves detailed consolidated outputs for each section and generates a refined markdown version of each chapter.

### Table of Contents

1. **Purpose and Core Functionality**
2. **Key Modules and Dependencies**
3. **Architectural Overview: The Multi-Agent System**
4. **Detailed Functional Breakdown**
    - Setup and Configuration
    - Agent Definitions (`TechnicalAccuracyAgent`, `ClarityAgent`, `StructureAgent`, `FactCheckingAgent`, `ModeratorAgent`)
    - `DebateManager`
    - Response Parsing Utilities (`extract_json_from_response`, `extract_last_asterisk_section`, `parse_improvement_points`)
    - Core Workflow Functions:
        - `assess_quality()`
        - `check_metric_thresholds()`
        - `get_needed_agents()`
        - `selective_review_section()`
        - `rewrite_section()` (from `rewrite_function.py`)
    - Output and File Handling (`save_consolidated_output`, `get_all_chapter_files`, `create_chapter_markdown`)
    - `main()` Orchestration
5. **Data Flow and Processing Pipeline**
6. **Key Concepts Employed**
    - Multi-Agent Systems (AutoGen)
    - Iterative Refinement and Self-Correction
    - Automated Quality Assessment
    - Selective Agent Engagement
    - Modular Design (Integration with `rewrite_function.py` and `evaluation_framework.py`)
7. **Technical Analysis**
    - Computational Efficiency
    - Model Quality & Robustness
    - Engineering Quality
8. **Conclusion and Potential Next Steps**

---

### 1. Purpose and Core Functionality

The central purpose of `step3.py` is to elevate the quality of automatically generated academic text from a previous stage (likely `step2.py`). It achieves this through:

1. **Automated Review:** Employing specialized LLM-based agents to analyze text from different perspectives (technical accuracy, clarity, structure, fact-checking).
2. **Consolidated Feedback:** Using a moderator agent to synthesize feedback from specialist agents into actionable improvement points.
3. **Targeted Rewriting:** Invoking an external rewriting function (`rewrite_text` from `rewrite_function.py`) to revise the text based on the consolidated feedback.
4. **Quantitative Evaluation:** Using an external evaluation framework (`evaluation_framework.py`, which in turn uses functions from `final_evaluation.py`) to score the text on various quality dimensions (technical depth, clarity, structure, citation accuracy).
5. **Iterative Improvement:** Repeating the review, rewrite, and evaluate cycle for a fixed number of iterations or until predefined quality thresholds are met. This allows the system to progressively refine the text.
6. **Selective Review:** In later iterations, only engaging agents whose corresponding quality metrics are below target, optimizing resource usage.
7. **Structured Output:** Saving detailed logs of the review and rewrite process for each section, along with the final improved text, in a consolidated JSON file.
8. **Markdown Generation:** Creating a final, refined markdown version of each chapter.

This script essentially simulates a peer-review and editing process using AI agents.

### 2. Key Modules and Dependencies

- **`autogen`**: The core library for creating and managing the multi-agent system.
- **`transformers`, `torch`, `huggingface_hub`**: For LLM loading (Gemma 27B via `CustomGemmaClient`) and inference, consistent with `step2.py` and `rewrite_function.py`.
- **`evaluation_framework` (local module, uses `final_evaluation.py`)**: Provides functions `calculate_technical_depth`, `calculate_clarity`, `calculate_structure`, `evaluate_citation_accuracy` to assess text quality.
- **`rewrite_function` (local module)**: Provides `rewrite_text`, `load_shared_model`, and `CustomGemmaClient` for LLM-based text rewriting.
- **Standard Libraries**: `json`, `os`, `pathlib`, `time`, `re`, `logging`, `gc`, `traceback`.
- **`dotenv`**: For loading environment variables.

The script cleverly extends the imported `CustomGemmaClient` from `rewrite_function.py` by adding a `cleanup` class method, demonstrating dynamic modification of imported classes.

### 3. Architectural Overview: The Multi-Agent System

`step3.py` orchestrates a team of AutoGen agents to refine text:

- **Specialist Agents**:
    - `TechnicalAccuracyAgent`: Focuses on factual correctness and technical depth.
    - `ClarityAgent`: Concentrates on readability, flow, and ease of understanding.
    - `StructureAgent`: Analyzes organization, coherence, and logical progression.
    - `FactCheckingAgent`: Verifies claims against provided reference paper content and checks citation integrity.
- **`ModeratorAgent`**: Acts as a synthesizer. It takes the raw feedback from all active specialist agents and produces a single, consolidated list of the most critical improvement points. This consolidated list guides the rewriting process.
- **`UserProxyAgent` (Implicit)**: AutoGen uses a `UserProxyAgent` to mediate interactions with agents, especially when `human_input_mode` is not "NEVER" or when initiating chats. Here, it's used with `human_input_mode="NEVER"` to automate the chat flows.

The general workflow within the agent system for a piece of text is:

1. The text is sent to selected specialist agents for review.
2. Each agent returns its feedback (a list of improvement suggestions).
3. The `ModeratorAgent` receives all these individual reviews.
4. The `ModeratorAgent` synthesizes them into a single, prioritized list of improvements.

This multi-agent "debate" or "consultation" followed by moderated synthesis is a powerful pattern for comprehensive text analysis.

### 4. Detailed Functional Breakdown

#### Setup and Configuration

- **Logging**: `setup_logging()` creates a timestamped log file in a `logs/` directory and also logs to the console.
- **GPU Memory Management**: Similar to `step2.py`, it sets `PYTORCH_CUDA_ALLOC_CONF` and `torch.backends.cuda.max_memory_split_size` for stable GPU memory usage with the large Gemma model.
- **Model Configuration (`OAI_CONFIG_LIST`)**: Defines the LLM configuration for AutoGen agents, specifying `google/gemma-3-27b-it` and the `CustomGemmaClient`. Generation parameters (`max_new_tokens`, `temperature`, etc.) are set.
- **Hugging Face Login**: Authenticates with Hugging Face.
- **`CustomGemmaClient.cleanup`**: A class method is dynamically added to the imported `CustomGemmaClient` to facilitate GPU memory cleanup specifically related to the shared model.

#### Agent Definitions

Each agent (`TechnicalAccuracyAgent`, `ClarityAgent`, `StructureAgent`, `FactCheckingAgent`, `ModeratorAgent`) is a subclass of `autogen.AssistantAgent`.

- **System Message**: The core of each agent's specialization is defined by its `system_message`. This prompt instructs the LLM on its role, the desired output format (a numbered list of 5-7 improvements between `***` markers), and its area of focus.
- **`review()` method**: Each specialist agent has a `review` method that takes the `section_name` and `section_content`. It constructs a prompt, initiates a chat with itself (a common AutoGen pattern for single-turn tasks), and extracts the response.
    - The `FactCheckingAgent`'s `review` method is more complex as it also takes `referenced_papers` and constructs a prompt that includes the reference content for the LLM to compare against.
- **LLM Configuration**: Agents are initialized with an `llm_config` that points to the `OAI_CONFIG_LIST`. They also register the `CustomGemmaClient`.
- **Output Format Expectation**: The agents are instructed to output their suggestions as a numbered list enclosed in `***` markers. This specific formatting is then parsed by helper functions.

**Example System Message (ClarityAgent):**

```
You are a clarity and readability specialist. Your role is to review content and provide a list of key improvements.

DO NOT use JSON format. DO NOT provide explanations before or after the list.
ONLY provide a numbered list of 5-7 specific, actionable improvements between *** markers.
...
Focus on clarity, readability, flow, and structure.
```

This direct instruction is key to getting structured feedback from the LLM through the agent.

#### `DebateManager`

A simple class to collect reviews from different agents.

- `add_review(self, agent_name: str, review: str)`: Appends a formatted review string.
- `resolve(self) -> str`: Combines all collected reviews into a single string, which is then passed to the `ModeratorAgent`.

#### Response Parsing Utilities

- **`extract_json_from_response(response: str)`**: Not heavily used by the core agent review logic in `step3.py` as agents are instructed _not_ to use JSON. It seems to be a general utility that might be used if agents were configured differently.
- **`extract_last_asterisk_section(text: str)`**: Crucial for extracting the list of improvements from the agent responses, as they are expected to be between `***` markers. It specifically targets the _last_ such section.
- **`parse_improvement_points(text: str)`**: Takes the text extracted by `extract_last_asterisk_section` and parses the numbered list into a Python `List[str]`.

#### Core Workflow Functions

##### `assess_quality(improved_text: str, original_text: str, referenced_papers: Dict = None, full_report_context: str = None) -> Dict`

- **Purpose**: Evaluates the quality of a piece of text using the imported evaluation functions.
- **Implementation**:
    - Calls `calculate_technical_depth`, `calculate_clarity`, and `calculate_structure` from the `evaluation_framework` (which relies on `final_evaluation.py`).
    - If `referenced_papers` are provided, it calls `evaluate_citation_accuracy`.
    - Checks for citation preservation by comparing sets of citation markers (e.g., `[1]`, `[2]`) between the original and improved text.
- **Output**: A dictionary containing detailed metrics for each quality dimension and citation analysis.

##### `check_metric_thresholds(metrics: Dict) -> Dict[str, bool]`

- **Purpose**: Compares the quality assessment scores against predefined thresholds.
- **Implementation**:
    - Defines thresholds for combined scores of technical depth, clarity, structure, and for overall/individual citation accuracy.
    - Returns a dictionary indicating whether each metric (and individual citations) meets its threshold.
- **Example Thresholds**:
    - Technical Depth (combined score): 0.8
    - Clarity (combined score): 0.7
    - Citation Accuracy (overall): 0.8
    - Citation Accuracy (individual): 0.7

##### `get_needed_agents(metric_results: Dict[str, bool]) -> List[str]`

- **Purpose**: Determines which specialist agents should be engaged in the next review iteration based on which quality metrics failed to meet their thresholds.
- **Implementation**:
    - If the 'technical_depth' overall score is below threshold, add `technical_accuracy_agent`.
    - If 'clarity' is below, add `clarity_agent`.
    - If 'structure' is below, add `structure_agent`.
    - If 'citation_accuracy' (overall or any individual citation) is below, add `fact_checking_agent`.
- **Rationale**: This function enables an adaptive review process. Instead of running all agents in every iteration after the first, it focuses the review efforts on areas that still need improvement, saving computational resources and time.

##### `selective_review_section(section_name: str, section_content: str, needed_agents: List[str], referenced_papers: Dict = None, previous_citation_scores: Dict = None) -> Dict`

- **Purpose**: Manages the review process for a single section, engaging only the `needed_agents`.
- **Implementation**:
    - Initializes only the agents specified in `needed_agents` list (plus the `ModeratorAgent`).
    - Each selected agent performs its `review()` method on the `section_content`.
        - The `FactCheckingAgent` receives `referenced_papers` to compare against.
    - The `ModeratorAgent` then synthesizes the reviews from the active agents.
    - It returns a dictionary containing the original content and the raw/cleaned reviews from all involved agents, including the moderator's synthesized guidance.
- **Note**: The `previous_citation_scores` parameter is passed but not explicitly used within this function's current logic for tailoring the `FactCheckingAgent`'s review beyond providing the standard reference material. Its presence suggests a potential for more nuanced fact-checking in future development.

##### `rewrite_section(section_text: str, improvements: List[str], model_config: Dict) -> str`

- This function now directly calls `rewrite_text` from the imported `rewrite_function.py`. It passes the original `section_text`, the `improvement_points` (formatted as a string), and LLM parameters.
- This decouples the rewriting logic from `step3.py`, making `rewrite_function.py` the dedicated module for this task.

#### Output and File Handling

- **`save_consolidated_output(data: Dict, section_name: str) -> str`**: Saves all iteration data for a section (reviews, improvement points, text before/after, quality assessments) to a structured JSON file in the `agent_outputs/` directory. It includes robust formatting for citation accuracy details.
- **`get_all_chapter_files() -> List[Path]`**: Scans the `initial_chapters/` directory (output of `step2.py`) for `chapter_*.json` files.
- **`create_chapter_markdown(chapter_number: int, sections: dict, consolidated_outputs: dict) -> None`**: Generates a final markdown file for each chapter in `chapter_markdowns/`. It takes the original sections (especially for "REFERENCES") and the finally improved text from `consolidated_outputs` for other sections. It also embeds citation accuracy analysis directly into the markdown.

#### `main()` Orchestration

The `main()` function drives the entire process:

1. **Load Chapters**: Gets all chapter JSON files from `initial_chapters/`.
2. **Process Chapters**: Iterates through each chapter.
    - Loads the chapter data (sections and referenced papers).
    - **Process Sections**: Iterates through each section in the chapter.
        - Skips the "REFERENCES" section to preserve it as is.
        - Initializes `current_text` with the original section content.
        - **Iterative Refinement Loop** (max 3 iterations):
            - **Iteration 1**: Uses all relevant agents (`technical_accuracy_agent`, `clarity_agent`, `structure_agent`, and `fact_checking_agent` if references exist) via `selective_review_section`.
            - **Subsequent Iterations**:
                - Calls `assess_quality()` on the `current_text`.
                - Calls `check_metric_thresholds()` to see if quality targets are met. If yes, breaks the loop for this section.
                - Calls `get_needed_agents()` to determine which agents to run. If none, breaks.
                - Calls `selective_review_section` with only the needed agents.
            - Extracts `improvement_points` from the moderator's synthesized review.
            - If no points, breaks.
            - Calls `rewrite_section` (which uses `rewrite_text` from `rewrite_function.py`) to get `improved_text`.
            - Updates `current_text` to `improved_text`.
            - Stores all data for the current iteration (reviews, points, text before/after, quality assessment) in `consolidated_output["iterations"]`.
        - After the loop (or if broken early), stores the `final_result` (final text, total iterations, final quality assessment) in `consolidated_output`.
        - Saves the `consolidated_output` for the section to a JSON file.
        - Adds the section's consolidated output to `chapter_consolidated_outputs`.
    - After processing all sections in a chapter, calls `create_chapter_markdown()` to generate the final `.md` file for the chapter.
    - Calls `CustomGemmaClient.cleanup()` to free GPU memory after each chapter.

### 5. Data Flow and Processing Pipeline

1. **Input**: `chapter_X.json` files from `initial_chapters/` (output of `step2.py`). Each file contains sections and referenced paper metadata.
2. **Chapter Loop**: For each chapter file:
    - **Section Loop**: For each section (e.g., "BACKGROUND KNOWLEDGE"):
        - **Initial State**: `current_text` = original section content.
        - **Iteration Loop (e.g., 1 to 3)**:
            - **(a) Review Phase**:
                - If Iteration 1 OR specific metrics are low: Engage selected/all specialist agents (`TechnicalAccuracyAgent`, `ClarityAgent`, etc.) via `selective_review_section`.
                - `FactCheckingAgent` uses `referenced_papers` data.
                - Agents provide feedback.
                - `ModeratorAgent` synthesizes feedback into `improvement_points`.
            - **(b) Rewrite Phase**:
                - `rewrite_section` (using `rewrite_function.rewrite_text`) revises `current_text` based on `improvement_points` -> `improved_text`.
                - `current_text` becomes `improved_text`.
            - **(c) Assess Phase**:
                - `assess_quality` evaluates `current_text` using `evaluation_framework.py` functions.
                - Metrics are checked against thresholds using `check_metric_thresholds`.
                - Decision for next iteration (continue, break, select specific agents) is made.
            - Log all iteration details.
        - **End of Section Processing**: Store final refined text and all iteration data for the section.
    - **End of Chapter Processing**:
        - `create_chapter_markdown` assembles the refined sections into a single markdown file.
        - GPU memory cleanup.
3. **Output**:
    - `agent_outputs/chapter_X_section_Y_consolidated.json`: Detailed JSON files for each processed section, logging all iterations.
    - `chapter_markdowns/chapter_X.md`: Final refined markdown files for each chapter.
    - `logs/improvement_process_TIMESTAMP.log`: Detailed log file.

**Diagram Description (Simplified Per-Section Iteration Flow):**

- Start: Original Section Text (from `step2.py` output)
- Loop (Iteration 1 to N):
    - Node 1: `selective_review_section`
        - Input: Current Section Text, Needed Agents
        - Sub-Process: Specialist Agents review -> Moderator synthesizes
        - Output: `improvement_points`
    - Node 2: `rewrite_section` (uses `rewrite_function.rewrite_text`)
        - Input: Current Section Text, `improvement_points`
        - Output: `improved_text`
    - (Update Current Section Text = `improved_text`)
    - Node 3: `assess_quality` (uses `evaluation_framework.py`)
        - Input: `improved_text`, Original Section Text, Referenced Papers
        - Output: Quality Metrics
    - Node 4: `check_metric_thresholds`
        - Input: Quality Metrics
        - Output: Decision (Stop / Continue / Select Agents for next iteration)
    - (If Stop or Max Iterations Reached, Exit Loop)
- End Loop
- Output: Final Refined Section Text, Consolidated Iteration Data (JSON)

### 6. Key Concepts Employed

- **Multi-Agent Systems (AutoGen)**: The script leverages AutoGen to create a collaborative environment where different AI agents specialize in specific aspects of text review. This allows for a more comprehensive and nuanced analysis than a single monolithic agent might provide.
- **Iterative Refinement (Self-Correction Loop)**: The core of the script is the iterative loop of review, rewrite, and evaluate. This allows the system to progressively improve the text, addressing issues identified in earlier iterations.
- **Automated Quality Assessment**: The integration of `evaluation_framework.py` (and thus `final_evaluation.py`) allows for quantitative measurement of text quality across multiple dimensions. This objective feedback is crucial for guiding the refinement process and determining when a section meets the desired quality.
- **Selective Agent Engagement**: The `get_needed_agents` function implements a form of adaptive resource allocation. By only activating agents relevant to sub-par metrics after the first iteration, the system can be more efficient.
- **Modular Design**: The script effectively integrates functionality from other modules:
    - `rewrite_function.py` for the actual text rewriting.
    - `evaluation_framework.py` (and `final_evaluation.py`) for quality assessment. This separation of concerns makes the overall system more maintainable and extensible.
- **Fact-Checking and Citation Accuracy**: A dedicated `FactCheckingAgent` and the `evaluate_citation_accuracy` metric highlight the importance of factual correctness and proper referencing in the generated academic-style text. The moderator is also explicitly instructed to prioritize these points.

### 7. Technical Analysis

#### Computational Efficiency

- **High Cost**: This script is computationally very expensive. Each iteration involves:
    - Multiple LLM calls for reviews by specialist agents.
    - An LLM call by the moderator agent.
    - An LLM call for rewriting the section.
    - Potential LLM calls within the `evaluation_framework` (e.g., for LLM-based clarity/technical depth scores in `final_evaluation.py`).
- Running this for multiple sections across multiple chapters, each for several iterations, requires significant GPU time and resources.
- The selective agent engagement in later iterations helps mitigate this cost somewhat.
- GPU memory management (`CustomGemmaClient.cleanup`, `gc.collect`, `torch.cuda.empty_cache`) is essential.

#### Model Quality & Robustness

- **Potential for Improvement**: The iterative process with targeted feedback has a strong potential to significantly improve the initial drafts from `step2.py`.
- **Error Handling**: The script includes `try-except` blocks at various levels (e.g., in `main` for chapter/section processing, within agent review methods).
- **Agent Response Parsing**: Relies on agents strictly following the `***` marker format. Failures in this can lead to empty or incorrect improvement points. The `extract_last_asterisk_section` and `parse_improvement_points` functions are designed to handle this.
- **Quality Thresholds**: The effectiveness of the process depends on well-calibrated quality thresholds.
- **Citation Preservation**: The `assess_quality` function checks if original citation markers are preserved, which is important. The `FactCheckingAgent` and `evaluate_citation_accuracy` further scrutinize citation usage.

#### Engineering Quality

- **Modularity**: Good separation of concerns with agents, evaluation, and rewriting.
- **Logging**: Comprehensive logging is implemented via the `setup_logging` function.
- **Configuration**: LLM configurations are centralized. Agent system messages clearly define roles.
- **AutoGen Use**: Demonstrates a practical application of a multi-agent framework for a complex task.
- **Code Readability**: The code is generally well-structured. Functions like `main` are long but follow a clear logic.
- **Resource Management**: Explicit calls to `CustomGemmaClient.cleanup()` after each chapter are a good practice.

### 8. Conclusion and Potential Next Steps

`step3.py` represents a sophisticated automated text refinement pipeline. By combining specialized AI agents for review, a moderator for synthesizing feedback, a dedicated rewriting module, and a quantitative evaluation framework, it systematically attempts to improve the quality of generated academic content. The iterative nature of the process, coupled with adaptive agent engagement, allows for targeted improvements.

**Potential Areas for Improvement or Extension:**

- **More Sophisticated Moderation**: The `ModeratorAgent` could be enhanced to resolve conflicting advice from specialist agents or to provide more nuanced weights to different types of feedback.
- **Advanced Stopping Criteria**: Beyond a fixed number of iterations and metric thresholds, consider criteria based on the rate of improvement (diminishing returns).
- **Human-in-the-Loop**: Integrate points where a human can review the moderator's suggestions or the rewritten text and provide feedback, especially for complex or ambiguous issues.
- **Fine-tuning Agents**: If specific, recurring issues are noted, the specialist agents themselves (or the LLMs powering them) could be fine-tuned for those tasks.
- **Deeper Fact-Checking**: The `FactCheckingAgent` primarily checks if claims are supported by _provided_ reference text. A more advanced version could attempt to verify claims against a broader knowledge base or search for external supporting/contradicting evidence.
- **Parallel Processing**: If resources allow, explore parallel processing of sections or even chapters, though GPU memory would be a major constraint.

This script is a significant step towards creating an autonomous system for generating and refining high-quality, multi-chapter documents.