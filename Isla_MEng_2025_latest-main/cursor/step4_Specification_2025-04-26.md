# Technical Specification: step4.py

**Date:** 2025-04-26

## 1. File Overview

* **Purpose:** This script serves as the final step in the report generation pipeline. Its primary function is to process the finalized chapter Markdown files (generated by `step3.py`) and create visual concept diagrams for each chapter using the Mermaid diagramming syntax. It leverages a large language model (Gemma 27B) to analyze the chapter content and generate the appropriate Mermaid code representing the key concepts and their relationships.
* **Role in Project:** Consumes the Markdown chapter files from `./chapter_markdowns/`, generates Mermaid diagrams for each, and saves these diagrams (as `.mmd` files) into the `./outputs/concept_diagrams/` directory. This provides a visual summary companion to the textual report chapters.

## 2. Key Classes

* No key classes are defined within this file. It primarily uses functions.

## 3. Key Functions

* **`setup_logging(log_level=logging.INFO)`**: Configures logging for the script, setting up both file (`./logs/concept_diagrams_{timestamp}.log`) and console handlers with specific formatting.
* **`load_model(model_name="google/gemma-3-27b-it", device="cuda")`**: Loads the specified Hugging Face model (Gemma 27B by default) and its tokenizer. It includes 4-bit quantization configuration (`BitsAndBytesConfig`), GPU memory settings (`max_memory`), error handling, and caching (`_model_cache`) to avoid reloading the model for each file. It also handles tokenizer setup (pad token).
* **`read_markdown_file(file_path)`**: Reads the content of a Markdown file (`.md`), attempts to extract the title from the first H1 heading (or derives it from the filename), and returns the title and the file content. Includes basic checks for minimum content length.
* **`generate_concept_diagram(model, tokenizer, title, content)`**: The core function that generates the Mermaid diagram.
  * Takes the loaded model, tokenizer, chapter title, and chapter content as input.
  * Truncates the input `content` if it exceeds a maximum length (`max_content_length` = 10000 characters) to fit within model constraints.
  * Constructs a detailed prompt instructing the LLM to act as a diagramming expert and generate a single, concise Mermaid diagram (flowchart, mindmap, etc.) visualizing 5-10 key concepts and their relationships from the provided text. The prompt specifically asks for *only* the Mermaid code block.
  * Uses the `model.generate()` method with specific parameters (`max_new_tokens`, `temperature`, `top_p`, `do_sample`) to produce the output.
  * Implements multiple robust methods to extract the Mermaid code block (```mermaid ...``` or ``` ... ```) from the potentially noisy LLM response, handling variations in formatting.
  * Validates the extracted Mermaid code (checks for diagram type keywords like `flowchart`, `graph`, `mindmap`).
  * Returns the cleaned Mermaid code string or `None` if generation or extraction fails.
* **`process_markdown_file(file_path, output_dir, model, tokenizer)`**: Orchestrates the processing for a single input Markdown file.
  * Calls `read_markdown_file` to get the title and content.
  * If content is valid, calls `generate_concept_diagram` to get the Mermaid code.
  * If Mermaid code is successfully generated, constructs the output filename (`{title}_concept_diagram.mmd`) and saves the code to the specified `output_dir`.
  * Handles potential errors during the process and logs outcomes.
* **`main()`**: The main execution function.
  * Parses command-line arguments (though none are explicitly defined in the provided snippet, suggesting potential future use or reliance on defaults).
  * Loads environment variables (`dotenv`).
  * Sets up input (`chapter_markdowns`) and output (`outputs/concept_diagrams`) directories, creating the output directory if necessary.
  * Loads the LLM and tokenizer using `load_model`.
  * Retrieves a list of all `.md` files from the input directory.
  * Iterates through each Markdown file using `tqdm` for progress tracking:
    * Calls `process_markdown_file` for the current file.
    * Includes a short sleep (`time.sleep(1)`) between files, possibly to manage resources or rate limiting (though less relevant for local models).
    * Clears CUDA cache periodically (`if i % 5 == 0: ... gc.collect()`) to manage GPU memory during the loop.
  * Logs the completion of the process.

## 4. Data Structures / Constants

* **Input Data Structure:** Reads Markdown (`.md`) files from `./chapter_markdowns/`.
* **Output Data Structure:** Writes Mermaid diagram code (`.mmd` files) to `./outputs/concept_diagrams/`.
* **Constants:** Default model name (`google/gemma-3-27b-it`), input/output directory paths, log file naming pattern, `max_content_length` for truncation, prompt template structure for diagram generation.
* **Internal Cache:** `_model_cache` (dictionary) used by `load_model` to store the loaded model and tokenizer.

## 5. Logic Flow & Control

* Script execution begins in `main()`.
* Initial setup: Argument parsing (potential), logging, environment loading, defining input/output paths, loading the shared LLM via `load_model`.
* The script finds all `.md` files in the `./chapter_markdowns/` directory.
* A main loop (`for file_path in tqdm(...)`) iterates through each discovered Markdown file.
* Inside the loop:
  * `process_markdown_file` is called.
    * Reads the file content (`read_markdown_file`).
    * If content is sufficient, generates the diagram (`generate_concept_diagram`).
      * This involves prompting the LLM and extracting the Mermaid code from the response.
    * If diagram generation is successful, saves the `.mmd` file to the output directory.
  * A brief pause occurs (`time.sleep(1)`).
  * Periodic GPU memory cleanup is performed (`gc.collect()`, `torch.cuda.empty_cache()`).
* Error handling (try-except blocks) is implemented in `read_markdown_file`, `load_model`, `generate_concept_diagram`, and `process_markdown_file` to gracefully handle issues with file reading, model loading, generation, or code extraction.

## 6. External Interactions

* **Imports:** `os`, `time`, `logging`, `argparse`, `sys`, `tqdm`, `pathlib`, `dotenv`, `torch`, `transformers`, `gc`.
* **File System Reads:**
  * `.env` file (via `dotenv`).
  * Markdown files (`.md`) from `./chapter_markdowns/`.
* **File System Writes:**
  * Log files to `./logs/`.
  * Mermaid diagram files (`.mmd`) to `./outputs/concept_diagrams/`.
  * Potentially writes model layers to `./offload/` if using `device_map="auto"` and offloading occurs.
* **External Libraries Called:**
  * `transformers`: Loading models (`AutoModelForCausalLM`), tokenizers (`AutoTokenizer`), quantization (`BitsAndBytesConfig`), generation (`model.generate`).
  * `torch`: Tensor operations, GPU management (`cuda.empty_cache`, memory stats, device setting).
  * `dotenv`: Loading `.env`.
  * `tqdm`: Displaying progress bar.
  * `pathlib`: Filesystem path manipulation.
  * `logging`: Recording execution details.
* **Local Modules Called:** None explicitly imported, relies on its own functions.
* **Exports/Intended Use by Others:** The primary output is the set of `.mmd` files in `./outputs/concept_diagrams/`. These files can be rendered by tools that support Mermaid syntax to visualize the chapter concepts.

## 7. Assumptions / Dependencies

* **Prior Steps:** Assumes `step3.py` has run successfully, producing the final chapter Markdown files in `./chapter_markdowns/`.
* **Environment:** Assumes a Python environment matching `environment.yml` (or similar) is active. Requires `transformers`, `torch` (with CUDA recommended), `dotenv`, `tqdm`, `accelerate`, `bitsandbytes`.
* **Hardware:** Strongly recommended to have a compatible NVIDIA GPU with sufficient VRAM (~70GiB configured in `load_model`) for the Gemma 27B model. CPU execution is possible but likely very slow.
* **API Keys/Tokens:** Assumes `HUGGINGFACE_TOKEN` might be needed if the model specified is gated (though Gemma models are typically open). `dotenv` is used, suggesting potential reliance on environment variables.
* **File Permissions:** Requires read access to `./chapter_markdowns/` and write access to `./outputs/concept_diagrams/`, `./logs/`, and potentially `./offload/`.
* **Model Behavior:** Assumes the LLM can follow the prompt instructions reasonably well to generate valid Mermaid syntax within the specified code block format. Relies on the extraction logic being able to handle the model's output format.
