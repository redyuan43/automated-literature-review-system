# Technical Specification: step1.py

**Date:** 2025-04-26

## 1. File Overview

* **Purpose:** This script serves as the first step in the document processing pipeline. Its primary responsibility is to read source documents (in Markdown format), extract relevant metadata, divide the content into manageable chunks, generate vector embeddings for these chunks using a pre-trained sentence transformer model, and finally, create and save a FAISS vector index along with associated metadata for efficient semantic searching in subsequent steps.
* **Role in Project:** It prepares the knowledge base (vector store) from raw source documents, which is essential input for the report generation phase (`step2.py`).

## 2. Key Classes

* No classes are defined within this file.

## 3. Key Functions

* **`extract_metadata(content: str, file_name: str) -> Dict[str, Any]`**
  * **Purpose:** Attempts to extract metadata (title, authors, year, abstract) from the raw text content of a Markdown file.
  * **Parameters:**
    * `content` (str): The full text content of the markdown file.
    * `file_name` (str): The base name of the file being processed.
  * **Return Value:** A dictionary containing the extracted metadata fields. Defaults are provided if extraction fails. The `id` field is set to the `file_name`.
  * **Core Logic:** Uses regular expressions (`re`) to search for common patterns associated with titles (markdown H1), authors (various "by Author1, Author2" formats), year (4-digit numbers), and abstracts ("Abstract:", "SUMMARY:", etc.).
* **`chunk_document(content: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]`**
  * **Purpose:** Splits the document content into smaller, potentially overlapping chunks suitable for embedding.
  * **Parameters:**
    * `content` (str): The full text content of the document.
    * `chunk_size` (int): The target maximum size (in characters) for each chunk. Defaults to `CHUNK_SIZE` constant (1000).
    * `overlap` (int): The number of characters to overlap between consecutive chunks. Defaults to `CHUNK_OVERLAP` constant (200).
  * **Return Value:** A list of strings, where each string is a text chunk.
  * **Core Logic:** Splits the content by paragraphs (`\\n\\s*\\n`). It iteratively adds paragraphs to the current chunk until adding the next paragraph would exceed `chunk_size`. When a chunk is finalized, it is added to the list, and the next chunk starts with the last `overlap` characters of the previous one (if `overlap > 0`).
* **`read_markdown_files(markdown_dir: str) -> List[Dict[str, Any]]`**
  * **Purpose:** Reads all `.mmd` files from a specified directory, processes each file to extract metadata and create chunks, and compiles a list of dictionaries, where each dictionary represents a single chunk with its associated metadata.
  * **Parameters:**
    * `markdown_dir` (str): The path to the directory containing the `.mmd` files (defaults to `MARKDOWN_DIR` constant './files_mmd').
  * **Return Value:** A list of dictionaries. Each dictionary contains details for one chunk, including `file_name`, `content` (the chunk text), `path`, `title`, `authors`, `year`, `abstract`, `chunk_id`, and `total_chunks`.
  * **Core Logic:** Uses `glob` to find all `.mmd` files. Iterates through each file, reads its content, calls `extract_metadata` and `chunk_document`, and then creates a dictionary entry for every resulting chunk, copying the document-level metadata to each chunk entry. Logs progress and errors.
* **`create_faiss_database(documents: List[Dict[str, Any]], output_dir: str)`**
  * **Purpose:** Takes the list of chunked documents, generates vector embeddings for each chunk's content using a SentenceTransformer model, builds a FAISS index for these embeddings, and saves the index and associated metadata.
  * **Parameters:**
    * `documents` (List[Dict[str, Any]]): The list of chunk dictionaries generated by `read_markdown_files`.
    * `output_dir` (str): The directory where the FAISS index and metadata files will be saved (defaults to `FAISS_DIR` constant './embeddings').
  * **Return Value:** Boolean indicating success (True) or failure (False).
  * **Core Logic:**
        1. Checks if the `documents` list is empty.
        2. Ensures the `output_dir` exists.
        3. Loads the specified SentenceTransformer model (`MODEL_NAME` constant "all-MiniLM-L6-v2").
        4. Iterates through the `documents` list (using `tqdm` for progress bar):
            *Encodes the `content` of each chunk using `model.encode()`.
            * Stores the resulting embedding and compiles corresponding metadata (including an excerpt).
        5. Converts the list of embeddings into a NumPy array (`float32`).
        6. Creates a FAISS `IndexFlatL2` index with the appropriate dimension.
        7. Adds the embeddings array to the FAISS index.
        8. Saves the FAISS index to `faiss.index` in the `output_dir`.
        9. Saves the collected metadata list as a NumPy array (`metadata.npy`) and also as a JSON file (`metadata.json`) in the `output_dir`.
        10. Saves indexing statistics (`stats.json`) to the `output_dir`.
        11. Logs progress and details throughout the process.
* **`main()`**
  * **Purpose:** Orchestrates the overall execution flow of the script.
  * **Parameters:** None.
  * **Return Value:** None.
  * **Core Logic:**
        1. Calls `read_markdown_files` to process documents from `MARKDOWN_DIR`.
        2. Logs the number of documents and chunks created.
        3. Calls `create_faiss_database` to build and save the index and metadata to `FAISS_DIR`.
        4. Logs the success or failure of the database creation.

## 4. Data Structures / Constants

* **Constants:**
  * `MARKDOWN_DIR` (str): `./files_mmd` - Directory containing input Markdown files.
  * `FAISS_DIR` (str): `./embeddings` - Directory for outputting the FAISS index and metadata.
  * `MODEL_NAME` (str): `all-MiniLM-L6-v2` - The Hugging Face identifier for the SentenceTransformer model used for embeddings.
  * `CHUNK_SIZE` (int): `1000` - Target character size for text chunks.
  * `CHUNK_OVERLAP` (int): `200` - Character overlap between consecutive text chunks.
* **Primary Data Structure:**
  * `chunked_documents` (List[Dict[str, Any]]): A list where each element is a dictionary representing a single text chunk. Key fields include:
    * `file_name` (str): Original source file name.
    * `content` (str): The text content of the chunk.
    * `path` (str): Full path to the original source file.
    * `title` (str): Extracted title of the source document.
    * `authors` (List[str]): Extracted authors of the source document.
    * `year` (str): Extracted year of the source document.
    * `abstract` (str): Extracted abstract of the source document.
    * `chunk_id` (int): The sequential ID of this chunk within its source document.
    * `total_chunks` (int): The total number of chunks created for the source document.
    * `excerpt` (str): (Added during `create_faiss_database`) First 500 characters of the chunk content.
    * `id` (str): (Added during `create_faiss_database`) Unique identifier string, typically `f"{file_name}_{chunk_id}"`.

## 5. Logic Flow & Control

* The script is intended to be run directly (`if __name__ == "__main__":`).
* The `main` function defines the primary execution sequence:
    1. Call `read_markdown_files` to process all `.mmd` files in the input directory. This function internally loops through files found by `glob` and calls `extract_metadata` and `chunk_document` for each.
    2. Call `create_faiss_database` with the resulting list of chunked documents. This function internally loops through the chunks (using `tqdm`) to generate embeddings before building and saving the index.
* Error handling (try-except blocks) is present within `read_markdown_files` to log errors for individual file processing and continue with others.
* Logging is configured at the beginning to output INFO level messages (and above) to both the console (`StreamHandler`) and a file (`document_indexing.log`).

## 6. External Interactions

* **Imports:** `os`, `sys`, `glob`, `re`, `json`, `logging`, `typing`, `numpy`, `faiss`, `sentence_transformers`, `tqdm`.
* **File System Reads:** Reads `.mmd` files from the directory specified by `MARKDOWN_DIR`.
* **File System Writes:**
  * Writes `faiss.index`, `metadata.npy`, `metadata.json`, `stats.json` to the directory specified by `FAISS_DIR`.
  * Writes log messages to `document_indexing.log`.
* **External Libraries Called:**
  * `glob.glob`: To find input files.
  * `re.search`, `re.findall`, `re.split`, `re.sub`: For metadata extraction and chunking logic.
  * `json.dump`: To save metadata and stats as JSON.
  * `logging`: For logging status and errors.
  * `numpy.array`, `numpy.save`: For handling embeddings array and saving metadata.
  * `faiss.IndexFlatL2`, `faiss.write_index`: To create and save the vector index.
  * `sentence_transformers.SentenceTransformer`: To load the embedding model and encode text chunks.
  * `tqdm.tqdm`: To display a progress bar during embedding generation.
* **Exports/Intended Use by Others:** While the script executes a self-contained process, its primary *output* (the FAISS index and metadata in `./embeddings`) is crucial input for `step2.py`. The functions themselves (`extract_metadata`, `chunk_document`, etc.) could potentially be imported and used by other modules, but the main execution flow suggests it's primarily run as a standalone first step.

## 7. Assumptions / Dependencies

* **Environment:** Assumes a Python environment where all imported libraries (`numpy`, `faiss`, `sentence-transformers`, `tqdm`) are installed and compatible.
* **Input Data:** Assumes the directory specified by `MARKDOWN_DIR` (`./files_mmd`) exists and contains files with the `.mmd` extension. Assumes these files are UTF-8 encoded. Assumes the content follows patterns that the regex in `extract_metadata` can potentially match.
* **Output Directory:** Assumes the script has write permissions to the directory specified by `FAISS_DIR` (`./embeddings`) and to the current working directory for the log file (`document_indexing.log`).
* **Model Availability:** Assumes the SentenceTransformer model specified by `MODEL_NAME` (`all-MiniLM-L6-v2`) can be downloaded or is available in the environment's cache.
* **Resource Availability:** Implicitly depends on sufficient CPU/Memory resources to load the embedding model, process documents, and build the index.
