# Technical Specification: final_evaluation.py\n\n**Date:** 2025-04-26\n\n## 1. File Overview\n\n***Purpose:** This module provides a suite of functions designed to quantitatively evaluate the quality of generated text, specifically academic or technical report sections. It assesses quality across four main dimensions: Technical Depth, Clarity, Structure, and Citation Accuracy. It combines traditional NLP metrics (readability scores, syntactic complexity, topic modeling, term frequency) with evaluations performed by a large language model (GPT-4 via OpenAI API) for a more holistic assessment.\n*   **Role in Project:** This module serves as the evaluation engine, primarily used by `step3.py` to assess the initial quality of generated sections and the quality after AI-assisted revisions. The scores generated help determine if a section requires improvement and track the effectiveness of the review/rewrite process.\n\n## 2. Key Classes\n\n***`ContextualCoherenceAnalyzer`**\n*   **Responsibility:** Analyzes the coherence and logical flow of text using sentence embeddings.\n    ***Key Attributes:** `nlp` (spaCy model), `model` (SentenceTransformer model).\n*   **Key Methods:**\n        *`__init__`: Loads the spaCy model (`en_core_web_sm`) and the SentenceTransformer model (`all-MiniLM-L6-v2`).\n*   `analyze_contextual_coherence`: Calculates sentence embeddings, computes cosine similarity between adjacent sentences, and derives coherence metrics (mean, std dev, abrupt changes). Returns a normalized coherence score (0-1).\n        *`analyze_concept_flow`: (Not directly called by main evaluation functions, possibly experimental) Splits text into chunks, calculates similarities between chunk centroids, and analyzes flow based on these similarities.\n*   `get_quality_label`: Maps a numerical score (0-1) to a qualitative label (e.g., \"Low\", \"Medium\", \"High\").\n    ***Core Functionality:** Provides quantitative measures of how well sentences or text chunks logically connect to each other, contributing to the overall `calculate_structure` score.\n\n## 3. Key Functions\n\n*   **`analyze_sentence_complexity_normalized(text, nlp)`**: Calculates the average maximum dependency tree depth per sentence using spaCy\'s parser. Normalizes this depth to a 0-1 score, where an optimal range (e.g., depth 6-7) receives the highest score (1.0), and scores decrease for overly simple or overly complex syntax. Used within `calculate_technical_depth`.\n***`evaluate_technical_depth_with_llm(text)`**: Prompts the OpenAI API (GPT-4) to evaluate the technical depth of the input text based on specific criteria (vocabulary, concept complexity, accuracy, sophistication). Uses a critical prompt designed to avoid overly generous scoring. Extracts the score (0-1) and justification from the LLM\'s JSON response. Handles API errors and parsing failures, returning a default score if needed. Used within `calculate_technical_depth`.\n*   **`calculate_technical_depth(text)`**: The main function for assessing technical depth.\n    *Loads a large, hardcoded `semiconductor_dictionary` of technical terms.\n*   Performs dictionary-based term counting (case-insensitive, handles multi-word terms). Normalizes this count based on text length.\n    *(Potentially uses spaCy NER, though the NER processing part seems incomplete in the provided snippet).\n*   Calls `analyze_sentence_complexity_normalized` for a syntax complexity score.\n    *Calls `evaluate_technical_depth_with_llm` for an LLM-based assessment.\n*   Combines the dictionary score, NER score (if implemented), syntax score, and LLM score using weighted averaging to produce a final score (0-1).\n    *Returns a dictionary containing the final `score` and the individual `components` scores/details.\n*   **`normalize_gunning_fog(gunning_fog)`**: Takes a Gunning Fog index value and normalizes it to a 0-1 score, where lower Fog indices (indicating easier readability) get higher scores. Used within `calculate_clarity`.\n***`evaluate_clarity_with_llm(text)`**: Prompts the OpenAI API (GPT-4) to evaluate the clarity and readability of the text (flow, conciseness, jargon use). Extracts the score (0-1) and justification. Handles errors. Used within `calculate_clarity`.\n*   **`calculate_clarity(text)`**: The main function for assessing clarity.\n    *Calculates the Gunning Fog index using `textstat.gunning_fog`.\n*   Normalizes the Fog index using `normalize_gunning_fog`.\n    *Calls `evaluate_clarity_with_llm` for an LLM-based assessment.\n*   Combines the normalized Fog score and the LLM score using weighted averaging.\n    *Returns a dictionary containing the final `score` and `components` details.\n*   **`analyze_topic_hierarchy_normalized(text, num_topics=5)`**: Performs Latent Dirichlet Allocation (LDA) topic modeling on the text using `sklearn`.\n    *Vectorizes the text using `CountVectorizer`.\n*   Fits an LDA model to identify `num_topics`.\n    *Calculates metrics related to topic coherence and distribution (e.g., perplexity, average topic probability). (Note: The exact metrics used for the final score calculation aren\'t fully shown in the snippet but likely relate to topic concentration/consistency).\n*   Returns a normalized score (0-1) reflecting topic structure quality. Used within `calculate_structure`.\n***`evaluate_structure_with_llm(text)`**: Prompts the OpenAI API (GPT-4) to evaluate the overall structure, organization, and logical flow of the text. Extracts the score (0-1) and justification. Handles errors. Used within `calculate_structure`.\n*   **`calculate_structure(text)`**: The main function for assessing structure.\n    *Instantiates `ContextualCoherenceAnalyzer` and calls `analyze_contextual_coherence` for a coherence score.\n*   Calls `analyze_topic_hierarchy_normalized` for a topic structure score.\n    *Calls `evaluate_structure_with_llm` for an LLM-based assessment.\n*   Combines the coherence, topic, and LLM scores using weighted averaging.\n    *Returns a dictionary containing the final `score` and `components` details.\n*   **`evaluate_citation_accuracy(text: str, referenced_papers: Dict)`**: The main function for assessing citation quality.\n    *Uses regex (`re.findall(r\'\\\[(\\d+)\\\]\', text)`) to find all numerical citation markers (e.g., \"[1]\") in the text.\n*   Compares the set of found citation numbers against the expected citation IDs derived from the `referenced_papers` input dictionary.\n    *Calculates `completeness` (ratio of expected citations found) and `correctness` (ratio of found citations that were expected).\n*   Prompts the OpenAI API (GPT-4) with the text, expected references, and found citations to assess contextual relevance and formatting. Extracts the LLM score.\n    *Combines completeness, correctness, and LLM scores using weighted averaging.\n*   Returns a dictionary containing the final `score`, `components` details, and lists of `found_citations`, `expected_citations`, `missing_citations`, and `unexpected_citations`.\n***`get_quality_label(score)`**: (Duplicate definition?) Simple utility function to map a numerical score (0-1) to a qualitative label (\"Low\", \"Medium\", \"High\").\n\n## 4. Data Structures / Constants\n\n*   **`semiconductor_dictionary` (Set[str]):** A large, hardcoded set of lowercase technical terms related to semiconductors, physics, materials, devices, and characterization methods. Used by `calculate_technical_depth`.\n***Input/Output Format (Main Functions):** All primary `calculate_*` functions take `text` (string) as input (plus `referenced_papers` for citation) and return a dictionary with a `score` (float 0-1) and a `components` dictionary detailing the sub-scores and LLM justifications. `evaluate_citation_accuracy` also returns lists of citation details.\n*   **LLM Prompt Templates:** Specific, detailed prompts are hardcoded within the `evaluate_*_with_llm` functions to guide the GPT-4 model for each quality dimension. These prompts include scoring guidelines.\n***Weights:** Implicit weighting factors are used within the `calculate_*` functions to combine scores from different components (e.g., NLP metrics + LLM score).\n\n## 5. Logic Flow & Control\n\n*   The module primarily provides functions intended to be imported and called by other scripts (`step3.py`). It does not have a `main` execution block for standalone running.\n*Each `calculate_*` function follows a similar pattern:\n    1.  Perform various analyses using NLP libraries (spaCy, textstat, sklearn) and/or custom logic (dictionary matching, regex).\n    2.  Call a dedicated helper function (`evaluate_*_with_llm`) to get an assessment from the OpenAI API (GPT-4). This involves constructing a prompt, making the API call, and parsing the JSON response. Error handling for the API call is included.\n    3.  Combine the scores from the NLP/custom analyses and the LLM evaluation using predetermined weights.\n    4.  Return the final score and component details in a structured dictionary.\n*   The `ContextualCoherenceAnalyzer` class encapsulates the logic and loaded models for coherence analysis.\n\n## 6. External Interactions\n\n***Imports:** `re`, `spacy`, `numpy`, `openai`, `json`, `textstat`, `typing`, `os`, `sklearn`, `dotenv`.\n*   **OpenAI API:** Makes calls to the `openai.chat.completions.create` endpoint using the `gpt-4` model to get qualitative evaluations for depth, clarity, structure, and citation accuracy. Requires an API key loaded via `dotenv`.\n***Local Modules Called:** None. This module provides evaluation functions *to* other modules (`step3.py`).\n*   **Exports/Intended Use by Others:** Exports the main evaluation functions: `calculate_technical_depth`, `calculate_clarity`, `calculate_structure`, `evaluate_citation_accuracy`. These are directly used by `step3.py`.\n***File System:** Reads the `.env` file to load the `OPENAI_API_KEY`.\n\n## 7. Assumptions / Dependencies\n\n*   **API Keys:** Assumes a valid `OPENAI_API_KEY` is available in the `.env` file or environment variables for the LLM evaluation calls to succeed. Warns if not set but doesn\'t prevent execution (LLM calls will fail).\n***Libraries:** Requires installation of `spacy` (and a model like `en_core_web_sm`), `numpy`, `openai`, `textstat`, `scikit-learn`, `python-dotenv`, `sentence-transformers`.\n*   **NLP Models:** Assumes the necessary spaCy model (`en_core_web_sm`) and SentenceTransformer model (`all-MiniLM-L6-v2`) are downloaded or can be downloaded.\n***Input Data:** Assumes the input `text` is a string. Assumes `referenced_papers` (for citation evaluation) is a dictionary with expected structure (mapping titles to details including `citation_id`).\n*   **LLM Availability & Performance:** Relies on the OpenAI API being available and the GPT-4 model performing reasonably well in following the structured JSON output format requested in the prompts. Includes basic parsing fallback.\n*   **Computational Resources:** NLP analyses (spaCy parsing, LDA, sentence embeddings) can be computationally intensive, especially for long texts
